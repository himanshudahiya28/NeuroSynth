{
  "cells": [
    {
      "cell_type": "code",
      "source": [
        "!pip install textstat\n",
        "!pip install fasttext\n",
        "!pip install gensim\n",
        "!pip install datasets"
      ],
      "metadata": {
        "id": "JUIK5D5nl4Kv"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import nltk\n",
        "nltk.download('words')\n",
        "nltk.download('wordnet')\n",
        "nltk.download('punkt')\n",
        "nltk.download('stopwords')\n",
        "nltk.download('cmudict')"
      ],
      "metadata": {
        "id": "NbWFpFGel8C-"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import re\n",
        "import numpy as np\n",
        "import textstat\n",
        "import spacy\n",
        "\n",
        "nlp = spacy.load('en_core_web_sm')\n",
        "\n",
        "def remove_possessive_markers(sentence):\n",
        "    if not isinstance(sentence, str):\n",
        "        sentence = str(sentence)\n",
        "    return re.sub(r\"'s|'\", \"\", sentence)\n",
        "\n",
        "def syllable_count(word):\n",
        "    return max(1, len(re.findall(r'[aeiouy]+', word, re.IGNORECASE)))\n",
        "\n",
        "def normalize_score(score, min_value, max_value):\n",
        "    return (score - min_value) / (max_value - min_value) if max_value != min_value else 0.5\n",
        "\n",
        "def calculate_sentence_complexity(sentence):\n",
        "    # Remove possessive markers\n",
        "    sentence = str(remove_possessive_markers(sentence))\n",
        "\n",
        "    # Tokenize the sentence\n",
        "    doc = nlp(sentence)\n",
        "    #tokens = nltk.word_tokenize(sentence)\n",
        "    tokens = [token for token in doc if not token.is_punct and not token.is_space\n",
        "              and not token.like_email and not token.like_url and not token.text.endswith('.')]\n",
        "\n",
        "    # Get unique lemmas\n",
        "    unique_lemmas = set(token.lemma_ for token in doc if token.text != '.')\n",
        "\n",
        "    # Calculate word metrics\n",
        "    word_metrics = [(token.text, syllable_count(token.text), 'complex' if syllable_count(token.text) >= 3 else 'simple') for token in tokens]\n",
        "\n",
        "    # Calculate total syllables, total words, and complex word count\n",
        "    total_syllables = sum(metric[1] for metric in word_metrics)\n",
        "    total_words = len(word_metrics)\n",
        "    complex_word_count = sum(1 for _, syllables, _ in word_metrics if syllables >= 3)\n",
        "\n",
        "    # Calculate metrics\n",
        "    total_complex_ratio_norm = complex_word_count / total_words if total_words != 0 else 0\n",
        "    max_dependency_depth = max(len(list(token.ancestors)) + 1 for token in doc)\n",
        "\n",
        "    # Normalize and calculate metrics\n",
        "    coleman_liau_index = normalize_score(textstat.coleman_liau_index(sentence), -8, 28)\n",
        "    gunning_fog_index = normalize_score(textstat.gunning_fog(sentence), 0, 32)\n",
        "    flesch_reading_ease_score = 1 - normalize_score(textstat.flesch_reading_ease(sentence), -73, 121)\n",
        "    dale_chall_readability_score = normalize_score(textstat.dale_chall_readability_score(sentence), 0, 20)\n",
        "    automated_readability_index = normalize_score(textstat.automated_readability_index(sentence), -5, 23)\n",
        "    vocabulary_complexity = len(unique_lemmas) / total_words if total_words != 0 else 0\n",
        "    sentence_length = total_words / 100\n",
        "    dependency_depth = normalize_score(max_dependency_depth, 1, 8)\n",
        "\n",
        "    # Calculate overall complexity score\n",
        "    overall_complexity_score = (\n",
        "        coleman_liau_index * 0.15 +\n",
        "        gunning_fog_index * 0.15 +\n",
        "        flesch_reading_ease_score * 0.15 +\n",
        "        dale_chall_readability_score * 0.15 +\n",
        "        automated_readability_index * 0.1 +\n",
        "        (len(set(token.text for token in doc)) / total_words if total_words != 0 else 0) * 0.1 +\n",
        "        total_complex_ratio_norm * 0.1 +\n",
        "        sentence_length * 0.05 +\n",
        "        dependency_depth * 0.05\n",
        "    )\n",
        "\n",
        "    # Return the numpy array of metric values\n",
        "    metric_values = np.array([\n",
        "        coleman_liau_index,\n",
        "        gunning_fog_index,\n",
        "        flesch_reading_ease_score,\n",
        "        dale_chall_readability_score,\n",
        "        automated_readability_index,\n",
        "        vocabulary_complexity,\n",
        "        total_complex_ratio_norm,\n",
        "        sentence_length,\n",
        "        dependency_depth,\n",
        "        overall_complexity_score\n",
        "    ])\n",
        "\n",
        "    return metric_values"
      ],
      "metadata": {
        "id": "baqQ_wWEl-fK"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import gensim.downloader as api\n",
        "glove_vectors = api.load('glove-wiki-gigaword-50')\n",
        "!pip install -q tfds-nightly tensorflow matplotlib"
      ],
      "metadata": {
        "id": "10USthLimEyd"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import matplotlib.pyplot as plt\n",
        "import numpy as np\n",
        "import tensorflow as tf"
      ],
      "metadata": {
        "id": "3yQN6rQTmLu-"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import tensorflow as tf\n",
        "import tensorflow_datasets as tfds\n",
        "import os\n",
        "import tempfile\n",
        "\n",
        "# Create a temporary directory for caching\n",
        "tmp_dir = tempfile.mkdtemp()\n",
        "\n",
        "# Load the training and validation data\n",
        "train_dataset = tfds.load('squad', split='train')\n",
        "val_dataset = tfds.load('squad', split='validation')\n",
        "\n",
        "def text_generator(dataset):\n",
        "    for example in dataset:\n",
        "        question = example['question'].numpy().decode('utf-8')\n",
        "        context = example['context'].numpy().decode('utf-8')\n",
        "        answer_start = example['answers']['answer_start'].numpy()[0]\n",
        "        answer_text = example['answers']['text'].numpy()[0].decode('utf-8')\n",
        "        title = example['title'].numpy().decode('utf-8')\n",
        "        id = example['id'].numpy().decode('utf-8')\n",
        "        yield question, context, answer_start, answer_text, title, id\n",
        "\n",
        "# Cache the pre-processed training data to disk\n",
        "cached_train_dataset = tf.data.Dataset.from_generator(\n",
        "    lambda: text_generator(train_dataset),\n",
        "    output_types=(tf.string, tf.string, tf.int32, tf.string, tf.string, tf.string)\n",
        ").cache(os.path.join(tmp_dir, 'train'))\n",
        "\n",
        "# Cache the pre-processed validation data to disk\n",
        "cached_val_dataset = tf.data.Dataset.from_generator(\n",
        "    lambda: text_generator(val_dataset),\n",
        "    output_types=(tf.string, tf.string, tf.int32, tf.string, tf.string, tf.string)\n",
        ").cache(os.path.join(tmp_dir, 'val'))\n",
        "\n",
        "# Iterate to cache data to disk\n",
        "cached_train_dataset = cached_train_dataset.prefetch(tf.data.AUTOTUNE)\n",
        "cached_val_dataset = cached_val_dataset.prefetch(tf.data.AUTOTUNE)"
      ],
      "metadata": {
        "id": "zN8UEa0nmYB1"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "decoded_data = []\n",
        "\n",
        "for question, context, answer_start, answer_text, title, id in cached_train_dataset:\n",
        "    question = question.numpy().decode('utf-8')\n",
        "    context = context.numpy().decode('utf-8')\n",
        "    answer_start = int(answer_start.numpy())\n",
        "    answer_text = answer_text.numpy().decode('utf-8')\n",
        "    title = title.numpy().decode('utf-8')\n",
        "    id = id.numpy().decode('utf-8')\n",
        "\n",
        "    decoded_data.append({\n",
        "        'question': question,\n",
        "        'context': context,\n",
        "        'answer_start': answer_start,\n",
        "        'answer_text': answer_text,\n",
        "        'title': title,\n",
        "        'id': id\n",
        "    })"
      ],
      "metadata": {
        "id": "_Y5U4wCpmZFm"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import numpy as np\n",
        "import torch\n",
        "from transformers import BasicTokenizer\n",
        "\n",
        "tokenizer = BasicTokenizer(do_lower_case=True)\n",
        "\n",
        "def print_max_length_examples(dataset):\n",
        "    max_question_length = 0\n",
        "    max_context_length = 0\n",
        "    max_question = None\n",
        "    max_context = None\n",
        "\n",
        "    for example in dataset:\n",
        "        question_tensor, context_tensor, _, _, _, _ = example\n",
        "        question_text = question_tensor.numpy().decode('utf-8')\n",
        "        context_text = context_tensor.numpy().decode('utf-8')\n",
        "        question_tokens = tokenizer.tokenize(question_text)\n",
        "        context_tokens = tokenizer.tokenize(context_text)\n",
        "        question_length = len(question_tokens)\n",
        "        context_length = len(context_tokens)\n",
        "\n",
        "        if question_length > max_question_length:\n",
        "            max_question_length = question_length\n",
        "            max_question = question_text\n",
        "\n",
        "        if context_length > max_context_length:\n",
        "            max_context_length = context_length\n",
        "            max_context = context_text\n",
        "\n",
        "    print(f\"Maximum question length: {max_question_length} tokens\")\n",
        "    print(\"Question with maximum length:\")\n",
        "    print(max_question)\n",
        "    print()\n",
        "\n",
        "    print(f\"Maximum context length: {max_context_length} tokens\")\n",
        "    print(\"Context with maximum length:\")\n",
        "    print(max_context)\n",
        "\n",
        "# Call the function with your datasets\n",
        "print_max_length_examples(cached_train_dataset)\n",
        "print_max_length_examples(cached_val_dataset)"
      ],
      "metadata": {
        "id": "uRQyPsEameuE"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import numpy as np\n",
        "import torch\n",
        "from transformers import BasicTokenizer\n",
        "\n",
        "tokenizer = BasicTokenizer(do_lower_case=True)\n",
        "\n",
        "def get_word_embedding(word, glove_vectors):\n",
        "    try:\n",
        "        return glove_vectors[word]\n",
        "    except KeyError:\n",
        "        return np.zeros(glove_vectors.vector_size)\n",
        "\n",
        "def prepare_input_features(question_str, context_str, glove_vectors, max_question_length=60, max_context_length=815):\n",
        "\n",
        "    # Tokenize questions and contexts\n",
        "    question_tokens = tokenizer.tokenize(question_str)\n",
        "    context_tokens = tokenizer.tokenize(context_str)\n",
        "\n",
        "    # Calculate metrics for question and context\n",
        "    question_metrics = calculate_sentence_complexity(question_str)\n",
        "    context_metrics = calculate_sentence_complexity(context_str)\n",
        "\n",
        "    # Prepare input features for question\n",
        "    question_embeddings = []\n",
        "    for word in question_tokens:\n",
        "        embedding = get_word_embedding(word, glove_vectors)\n",
        "        combined_feature = np.concatenate([embedding, question_metrics])\n",
        "        question_embeddings.append(combined_feature)\n",
        "\n",
        "    # Pad or truncate the question_embeddings to the maximum question length across the dataset\n",
        "    padded_question_embeddings = np.zeros((max_question_length, combined_feature.shape[-1]))\n",
        "    for i, embedding in enumerate(question_embeddings):\n",
        "        padded_question_embeddings[i, :embedding.shape[0]] = embedding\n",
        "\n",
        "    # Prepare input features for context\n",
        "    context_embeddings = []\n",
        "    for word in context_tokens:\n",
        "        embedding = get_word_embedding(word, glove_vectors)\n",
        "        combined_feature = np.concatenate([embedding, context_metrics])\n",
        "        context_embeddings.append(combined_feature)\n",
        "\n",
        "    # Pad or truncate the context_embeddings to the maximum context length across the dataset\n",
        "    padded_context_embeddings = np.zeros((max_context_length, combined_feature.shape[-1]))\n",
        "    for i, embedding in enumerate(context_embeddings):\n",
        "        padded_context_embeddings[i, :embedding.shape[0]] = embedding\n",
        "\n",
        "    # Convert the numpy arrays to PyTorch tensors\n",
        "    question_features_tensor = torch.tensor(padded_question_embeddings)\n",
        "    context_features_tensor = torch.tensor(padded_context_embeddings)\n",
        "\n",
        "    return {'question_features': question_features_tensor, 'context_features': context_features_tensor}\n",
        "\n",
        "# Iterate over the first 8000 examples from the training set\n",
        "train_features = []\n",
        "for i, example in enumerate(decoded_data):\n",
        "    if i >= 8000:\n",
        "        break\n",
        "    question_str = example['question']\n",
        "    context_str = example['context']\n",
        "    features = prepare_input_features(question_str, context_str, glove_vectors)\n",
        "    train_features.append(features)\n",
        "\n",
        "# Iterate over the first 2000 examples from the validation set\n",
        "val_features = []\n",
        "for i, example in enumerate(decoded_data):\n",
        "    if i >= 2000:\n",
        "        break\n",
        "    question_str = example['question']\n",
        "    context_str = example['context']\n",
        "    features = prepare_input_features(question_str, context_str, glove_vectors)\n",
        "    val_features.append(features)"
      ],
      "metadata": {
        "id": "SrlJkesVmr9N"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import torch\n",
        "import torch.nn as nn\n",
        "import tensorflow as tf\n",
        "import tensorflow_datasets as tfds\n",
        "from transformers import BertTokenizer, BertForQuestionAnswering, BertConfig, BertModel\n",
        "from tqdm import tqdm\n",
        "from transformers import RobertaTokenizerFast\n",
        "\n",
        "class CustomEmbeddingLayer(nn.Module):\n",
        "    def __init__(self, output_dim):\n",
        "        super(CustomEmbeddingLayer, self).__init__()\n",
        "        self.projection = nn.Linear(self.get_input_dim(), output_dim)\n",
        "\n",
        "    def get_input_dim(self):\n",
        "        return self.current_input_shape[-1]\n",
        "\n",
        "    def forward(self, x):\n",
        "        self.current_input_shape = x.shape\n",
        "        projected_embeddings = self.projection(x)\n",
        "        return projected_embeddings\n",
        "\n",
        "\n",
        "# Custom Question Answering Model\n",
        "class CustomQAModel(nn.Module):\n",
        "    def __init__(self, config):\n",
        "        super(CustomQAModel, self).__init__()\n",
        "        self.bert = BertModel(config)\n",
        "        self.hidden_size = config.hidden_size\n",
        "        self.qa_outputs = nn.Linear(self.hidden_size, 2)\n",
        "\n",
        "    def forward(self, input_dict):\n",
        "        input_ids = input_dict['input_ids'].reshape(batch_size, 512)\n",
        "        attention_mask = input_dict['attention_mask']\n",
        "        token_type_ids = None  # BERT doesn't require token_type_ids\n",
        "        question_features_tensor = input_dict['question_features']\n",
        "        context_features_tensor = input_dict['context_features']\n",
        "\n",
        "        print(\"DEBUG: Input Shape to BertModel:\", input_ids.shape)\n",
        "\n",
        "        # *** Integration Point ***\n",
        "        outputs = self.bert(input_ids=input_ids, attention_mask=attention_mask, token_type_ids=token_type_ids)\n",
        "        sequence_output = outputs.last_hidden_state\n",
        "\n",
        "        question_features_tensor = question_features_tensor.unsqueeze(1)\n",
        "        context_features_tensor = context_features_tensor.unsqueeze(1)\n",
        "\n",
        "        combined_question_embeddings = torch.concat([sequence_output, question_features_tensor], dim=-1)\n",
        "        combined_context_embeddings = torch.concat([sequence_output, context_features_tensor], dim=-1)\n",
        "\n",
        "        question_output = self.qa_outputs(combined_question_embeddings)\n",
        "        context_output = self.qa_outputs(combined_context_embeddings)\n",
        "\n",
        "        start_logits, end_logits = question_output, context_output\n",
        "        start_logits = start_logits.squeeze(-1).contiguous()\n",
        "        end_logits = end_logits.squeeze(-1).contiguous()\n",
        "\n",
        "        total_loss = None\n",
        "        if start_positions is not None and end_positions is not None:\n",
        "            ignored_index = start_logits.size(-1)\n",
        "            start_positions = start_positions.clamp(0, ignored_index)\n",
        "            end_positions = end_positions.clamp(0, ignored_index)\n",
        "\n",
        "            loss_fct = nn.CrossEntropyLoss(ignore_index=ignored_index)\n",
        "            start_loss = loss_fct(start_logits, start_positions)\n",
        "            end_loss = loss_fct(end_logits, end_positions)\n",
        "            total_loss = (start_loss + end_loss) / 2\n",
        "\n",
        "        return total_loss, start_logits, end_logits\n",
        "\n",
        "# Load and preprocess the SQuAD dataset\n",
        "def load_preprocess_squad(decoded_data):\n",
        "    tokenizer = BertTokenizer.from_pretrained('bert-base-uncased')\n",
        "\n",
        "    def preprocess_squad(example):\n",
        "        question_str = example['question']\n",
        "        context_str = example['context']\n",
        "        answer_start = example['answer_start']\n",
        "        answer_text = example['answer_text']\n",
        "\n",
        "        # Tokenize question and context\n",
        "        inputs = tokenizer(question_str, context_str, max_length=512, truncation=True, padding='max_length', return_tensors='pt')\n",
        "\n",
        "        if isinstance(answer_start, int) and answer_start >= 0:\n",
        "            start_positions = answer_start\n",
        "        else:\n",
        "            start_positions = 0\n",
        "\n",
        "        if isinstance(answer_text, str) and len(answer_text) > 0:\n",
        "            end_positions = answer_start + len(answer_text) - 1\n",
        "        else:\n",
        "            end_positions = 0\n",
        "\n",
        "\n",
        "        input_ids = inputs['input_ids'].squeeze(0)\n",
        "        attention_mask = inputs['attention_mask'].squeeze(0)\n",
        "\n",
        "        return {\n",
        "            'input_ids': inputs['input_ids'],\n",
        "            'attention_mask': inputs['attention_mask'],\n",
        "            'start_positions': start_positions,\n",
        "            'end_positions': end_positions,\n",
        "            'question_features': question_features_tensor,\n",
        "            'context_features': context_features_tensor\n",
        "        }\n",
        "\n",
        "\n",
        "    # Preprocess examples from decoded_data\n",
        "    processed_examples = [preprocess_squad(example) for example in decoded_data]\n",
        "\n",
        "    # Split dataset into training and validation sets\n",
        "    train_dataset = processed_examples[:8000]\n",
        "    eval_dataset = processed_examples[8000:10000]\n",
        "\n",
        "    return train_dataset, eval_dataset\n",
        "\n",
        "# Training and evaluation functions\n",
        "def train_epoch(model, train_dataloader, optimizer):\n",
        "    model.train()\n",
        "    total_loss = 0\n",
        "    for batch in tqdm(train_dataloader, desc='Training'):\n",
        "        batch_dict = {}\n",
        "        print(\"Contents of the 'batch' dictionary (shapes):\")\n",
        "        for k, v in batch.items():\n",
        "            if isinstance(v, list):\n",
        "                batch_dict[k] = torch.tensor(v, device=device)\n",
        "            elif isinstance(v, torch.Tensor):\n",
        "                batch_dict[k] = v.to(device)\n",
        "            else:\n",
        "                batch_dict[k] = torch.tensor([v], device=device)\n",
        "        loss, _, _ = model(batch_dict)\n",
        "        total_loss += loss.item()\n",
        "        loss.backward()\n",
        "        optimizer.step()\n",
        "        optimizer.zero_grad()\n",
        "    return total_loss / len(train_dataloader)\n",
        "\n",
        "def eval_model(model, eval_dataloader):\n",
        "    model.eval()\n",
        "    total_loss = 0\n",
        "    for batch in eval_dataloader:\n",
        "        batch_dict = {}\n",
        "        for k, v in batch.items():\n",
        "            if isinstance(v, list):\n",
        "                batch_dict[k] = torch.tensor(v, device=device)\n",
        "            elif isinstance(v, torch.Tensor):\n",
        "                batch_dict[k] = v.to(device)\n",
        "            else:\n",
        "                batch_dict[k] = torch.tensor([v], device=device)\n",
        "        with torch.no_grad():\n",
        "            loss, start_logits, end_logits = model(batch_dict)\n",
        "        total_loss += loss.item()\n",
        "    return total_loss / len(eval_dataloader)\n",
        "\n",
        "# Main training loop\n",
        "def train_model(model, train_dataloader, eval_dataloader, optimizer, num_epochs=3):\n",
        "    best_val_loss = float('inf')\n",
        "    for epoch in range(num_epochs):\n",
        "        train_loss = train_epoch(model, train_dataloader, optimizer)\n",
        "        val_loss = eval_model(model, eval_dataloader)\n",
        "        print(f'Epoch {epoch+1}/{num_epochs}, Train Loss: {train_loss}, Val Loss: {val_loss}')\n",
        "\n",
        "        if val_loss < best_val_loss:\n",
        "            best_val_loss = val_loss\n",
        "            torch.save(model.state_dict(), 'best_model.pth')\n",
        "\n",
        "# Load datasets and model\n",
        "batch_size = 8\n",
        "train_dataset, eval_dataset = load_preprocess_squad(decoded_data)\n",
        "train_dataloader = torch.utils.data.DataLoader(train_dataset, batch_size=batch_size, shuffle=True)\n",
        "eval_dataloader = torch.utils.data.DataLoader(eval_dataset, batch_size=batch_size)\n",
        "device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
        "model_config = BertConfig.from_pretrained('bert-base-uncased')\n",
        "model = CustomQAModel(config=model_config)\n",
        "model.bert.load_state_dict(BertModel.from_pretrained('bert-base-uncased').state_dict())\n",
        "model.to(device)\n",
        "optimizer = torch.optim.AdamW(model.parameters(), lr=5e-5)\n",
        "\n",
        "# Train the model\n",
        "train_model(model, train_dataloader, eval_dataloader, optimizer)"
      ],
      "metadata": {
        "id": "bY7lo-Pjmvb3"
      },
      "execution_count": null,
      "outputs": []
    }
  ],
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}